{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a dataset, for this we're using Cotho a labled audio dataset. https://github.com/Labbeti/aac-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aac_datasets import Clotho\n",
    "dataset = Clotho(root=\"./datasets/\", download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Mulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram yielded shape of (65, 11026), but had to be cropped to (64, 11024) to be patchified for transformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import *\n",
    "mulan, audio_transformer, text_transformer = build_mulan()\n",
    "\n",
    "wavs = torch.randn(1, 44100*3)\n",
    "# texts = torch.randint(0, 20000, (2, 256))\n",
    "texts = [\"test text here\"]\n",
    "\n",
    "loss = mulan(wavs, raw_texts=texts)\n",
    "loss.backward()\n",
    "\n",
    "# # after much training, you can embed sounds and text into a joint embedding space\n",
    "# # for conditioning the audio LM\n",
    "\n",
    "torch.save(mulan.state_dict(), './models/mulan/ckpt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload the trained mulan and train the basics of the semantics transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 20:32:34 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 3647 samples and validating with randomly splitted 192 samples\n",
      "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
      "0: loss: 6.407083511352539\n",
      "0: valid loss 7.012447834014893\n",
      "0: saving model to models/SemanticTransformer\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from audiolm_pytorch import SemanticTransformerTrainer\n",
    "import torch\n",
    "\n",
    "mulan, _, _ = build_mulan()\n",
    "mulan.load_state_dict(torch.load('./models/mulan/ckpt.pt'))\n",
    "\n",
    "semantic_transformer, wav2vec, quantizer = build_semantic_transformer(mulan)\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
    "    folder ='./datasets/CLOTHO_v2.1/clotho_audio_files',\n",
    "    results_folder='./models/SemanticTransformer',\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 20:34:47 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import EncodecWrapper\n",
    "encodec = EncodecWrapper()\n",
    "# Now you can use the encodec variable in the same way you'd use the soundstream variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 21:58:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 3647 samples and validating with randomly splitted 192 samples\n",
      "0: soundstream total loss: 22.406, soundstream recon loss: 0.021 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to models/SoundStream\n",
      "0: saving model to models/SoundStream\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStreamTrainer\n",
    "from accelerate import Accelerator\n",
    "from models import *\n",
    "import torch\n",
    "\n",
    "soundstream = build_sound_stream()\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder ='./datasets/CLOTHO_v2.1/clotho_audio_files',\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length_seconds = 2,  # train on 2 second audio\n",
    "    num_train_steps = 1,\n",
    "    results_folder='./models/SoundStream',\n",
    "    accelerator=Accelerator(cpu=True) #Remove when training on GPU\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 00:17:50 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic:   1%|▏         | 26/2048 [00:00<00:50, 40.20it/s]\n",
      "generating coarse: 100%|██████████| 512/512 [05:55<00:00,  1.44it/s]\n",
      "generating fine:  97%|█████████▋| 498/512 [15:46<00:44,  3.15s/it]"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, AudioLM\n",
    "from accelerate import Accelerator\n",
    "from models import *\n",
    "import torch\n",
    "from musiclm_pytorch import MusicLM\n",
    "use_gpu = False\n",
    "\n",
    "mulan, _, _ = build_mulan()\n",
    "mulan.load_state_dict(torch.load('./models/mulan/ckpt.pt'))\n",
    "if use_gpu:\n",
    "    mulan = mulan.cuda()\n",
    "\n",
    "wav2vec = build_wav2vec()\n",
    "if use_gpu:\n",
    "    wav2vec = wav2vec.cuda()\n",
    "\n",
    "quantizer = build_quantizer(mulan)\n",
    "if use_gpu:\n",
    "    quantizer = quantizer.cuda()\n",
    "\n",
    "semantic_transformer = build_semantic_transformer(quantizer, wav2vec)\n",
    "semantic_transformer.load('./models/SemanticTransformer/semantic.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    semantic_transformer = semantic_transformer.cuda()\n",
    "\n",
    "\n",
    "coarse_transformer = build_coarse_transformer(wav2vec)\n",
    "coarse_transformer.load('./models/CoarseTransformer/coarse.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    coarse_transformer = coarse_transformer.cuda()\n",
    "\n",
    "\n",
    "fine_transformer = build_fine_transformer()\n",
    "fine_transformer.load('./models/FineTransformer/fine.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    fine_transformer = fine_transformer.cuda()\n",
    "\n",
    "\n",
    "soundstream = SoundStream.init_and_load_from('./models/SoundStream/soundstream.0.pt')\n",
    "if use_gpu:\n",
    "    soundstream = soundstream.cuda()\n",
    "\n",
    "\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "if use_gpu:\n",
    "    audiolm = audiolm.cuda()\n",
    "\n",
    "\n",
    "musiclm = MusicLM(\n",
    "    audio_lm = audiolm,                 # `AudioLM` from https://github.com/lucidrains/audiolm-pytorch\n",
    "    mulan_embed_quantizer = quantizer    # the `MuLaNEmbedQuantizer` from above\n",
    ")\n",
    "\n",
    "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 4) # sample 4 and pick the top match with mulan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
