{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a dataset, for this we're using Cotho a labled audio dataset. https://github.com/Labbeti/aac-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmaat/anaconda3/envs/musiclm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aac_datasets import Clotho\n",
    "dataset = Clotho(root=\"./datasets/\", subset=\"dev\", download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aac_datasets.utils.collate import BasicCollate\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=18, pin_memory=True, collate_fn=BasicCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 904050] at entry 0 and [1, 1270080] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 904050] at entry 0 and [1, 1270080] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 904050]),\n",
       " ['Some random tones are played continuously and repetitively.'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "audio, text = batch['audio'][0][0][None, :], [random.choice(batch['captions'][0])]\n",
    "audio.shape, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ffmpeg', 'soundfile']\n"
     ]
    }
   ],
   "source": [
    "dataset[0]\n",
    "import torchaudio\n",
    "print(torchaudio.list_audio_backends())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SOUNDFILE_LIBRARY\"] = \"/var/scratch/bmt270/miniconda3/envs/musiclm/lib/python3.10/site-packages/_soundfile_data/libsndfile_x86_64.so\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Mulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 16:13:29 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram yielded shape of (65, 11026), but had to be cropped to (64, 11024) to be patchified for transformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import *\n",
    "mulan, audio_transformer, text_transformer = build_mulan()\n",
    "\n",
    "wavs = torch.randn(1, 44100*3)\n",
    "# texts = torch.randint(0, 20000, (2, 256))\n",
    "texts = [\"test text here\"]\n",
    "\n",
    "loss = mulan(wavs, raw_texts=texts)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3025851249694824, 0.0, 0.0, 0.35081106424331665, 1.4202189445495605)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulan.contrast.temperatures.mean().item(), mulan.contrast.denominator_i.mean().item(), mulan.contrast.denominator_j.mean().item(), mulan.contrast.sims.mean().item(), mulan.contrast.numerator.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload the trained mulan and train the basics of the semantics transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 20:32:34 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 3647 samples and validating with randomly splitted 192 samples\n",
      "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
      "0: loss: 6.407083511352539\n",
      "0: valid loss 7.012447834014893\n",
      "0: saving model to models/SemanticTransformer\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from audiolm_pytorch import SemanticTransformerTrainer\n",
    "import torch\n",
    "\n",
    "mulan, _, _ = build_mulan()\n",
    "mulan.load_state_dict(torch.load('./models/mulan/ckpt.pt'))\n",
    "\n",
    "semantic_transformer, wav2vec, quantizer = build_semantic_transformer(mulan)\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
    "    folder ='./datasets/CLOTHO_v2.1/clotho_audio_files',\n",
    "    results_folder='./models/SemanticTransformer',\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 20:34:47 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import EncodecWrapper\n",
    "encodec = EncodecWrapper()\n",
    "# Now you can use the encodec variable in the same way you'd use the soundstream variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 21:58:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 3647 samples and validating with randomly splitted 192 samples\n",
      "0: soundstream total loss: 22.406, soundstream recon loss: 0.021 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to models/SoundStream\n",
      "0: saving model to models/SoundStream\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStreamTrainer\n",
    "from accelerate import Accelerator\n",
    "from models import *\n",
    "import torch\n",
    "\n",
    "soundstream = build_sound_stream()\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder ='./datasets/CLOTHO_v2.1/clotho_audio_files',\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length_seconds = 2,  # train on 2 second audio\n",
    "    num_train_steps = 1,\n",
    "    results_folder='./models/SoundStream',\n",
    "    accelerator=Accelerator(cpu=True) #Remove when training on GPU\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 00:17:50 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic:   1%|▏         | 26/2048 [00:00<00:50, 40.20it/s]\n",
      "generating coarse: 100%|██████████| 512/512 [05:55<00:00,  1.44it/s]\n",
      "generating fine:  97%|█████████▋| 498/512 [15:46<00:44,  3.15s/it]"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, AudioLM\n",
    "from accelerate import Accelerator\n",
    "from models import *\n",
    "import torch\n",
    "from musiclm_pytorch import MusicLM\n",
    "use_gpu = False\n",
    "\n",
    "mulan, _, _ = build_mulan()\n",
    "mulan.load_state_dict(torch.load('./models/mulan/ckpt.pt'))\n",
    "if use_gpu:\n",
    "    mulan = mulan.cuda()\n",
    "\n",
    "wav2vec = build_wav2vec()\n",
    "if use_gpu:\n",
    "    wav2vec = wav2vec.cuda()\n",
    "\n",
    "quantizer = build_quantizer(mulan)\n",
    "if use_gpu:\n",
    "    quantizer = quantizer.cuda()\n",
    "\n",
    "semantic_transformer = build_semantic_transformer(quantizer, wav2vec)\n",
    "semantic_transformer.load('./models/SemanticTransformer/semantic.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    semantic_transformer = semantic_transformer.cuda()\n",
    "\n",
    "\n",
    "coarse_transformer = build_coarse_transformer(wav2vec)\n",
    "coarse_transformer.load('./models/CoarseTransformer/coarse.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    coarse_transformer = coarse_transformer.cuda()\n",
    "\n",
    "\n",
    "fine_transformer = build_fine_transformer()\n",
    "fine_transformer.load('./models/FineTransformer/fine.transformer.0.pt')\n",
    "if use_gpu:\n",
    "    fine_transformer = fine_transformer.cuda()\n",
    "\n",
    "\n",
    "soundstream = SoundStream.init_and_load_from('./models/SoundStream/soundstream.0.pt')\n",
    "if use_gpu:\n",
    "    soundstream = soundstream.cuda()\n",
    "\n",
    "\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "if use_gpu:\n",
    "    audiolm = audiolm.cuda()\n",
    "\n",
    "\n",
    "musiclm = MusicLM(\n",
    "    audio_lm = audiolm,                 # `AudioLM` from https://github.com/lucidrains/audiolm-pytorch\n",
    "    mulan_embed_quantizer = quantizer    # the `MuLaNEmbedQuantizer` from above\n",
    ")\n",
    "\n",
    "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 4) # sample 4 and pick the top match with mulan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musiclm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
